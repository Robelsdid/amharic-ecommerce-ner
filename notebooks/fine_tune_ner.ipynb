{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf0b445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-output\",\n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc27b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05608e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file written to ../data/processed/ner_labels_template_cleaned.conll\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_path = '../data/processed/ner_labels_template.conll'\n",
    "output_path = '../data/processed/ner_labels_template_cleaned.conll'\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        # Fix common label typos\n",
    "        line = re.sub(r'B-CONTACT_INFO\\\\\\\\?', 'B-CONTACT_INFO', line)\n",
    "        line = re.sub(r'I-PRICErocessor', 'I-PRICE', line)\n",
    "        line = re.sub(r'B-PRICEroduct', 'B-Product', line)\n",
    "        line = re.sub(r'I-PRICEroduct', 'I-Product', line)\n",
    "        # Remove lines with 'Channel' as a label\n",
    "        if re.search(r'\\bChannel\\b', line):\n",
    "            continue\n",
    "        outfile.write(line)\n",
    "print(\"Cleaned file written to\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34c23f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 104 messages\n"
     ]
    }
   ],
   "source": [
    "def parse_conll(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    tokens.append(splits[0])\n",
    "                    tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "output_path = '../data/processed/ner_labels_template_cleaned.conll'  \n",
    "sentences, tags = parse_conll(output_path)\n",
    "print(f\"Loaded {len(sentences)} messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c8ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-CONTACT_INFO': 0, 'B-LOC': 1, 'B-PRICE': 2, 'B-Product': 3, 'B-QUANTITY': 4, 'B-SPECIFICATION': 5, 'I-CONTACT_INFO': 6, 'I-LOC': 7, 'I-PRICE': 8, 'I-Product': 9, 'O': 10}\n"
     ]
    }
   ],
   "source": [
    "data = {'tokens': sentences, 'ner_tags': tags}\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "label_list = sorted(list({l for tag_seq in tags for l in tag_seq}))\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ef93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb8de7c1a63425584964b0f18f0a1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d56b97150af439485b02b14e6b5ddc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-base\"  # or \"Davlan/bert-tiny-amharic\" or \"Davlan/afroxlmr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41729c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029960b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3cbff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aweso\\AppData\\Local\\Temp\\ipykernel_44800\\2117041618.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 07:38, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.374900</td>\n",
       "      <td>1.582383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.378500</td>\n",
       "      <td>1.286430</td>\n",
       "      <td>0.236967</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142045</td>\n",
       "      <td>0.584034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>1.006951</td>\n",
       "      <td>0.279365</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.709034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.881200</td>\n",
       "      <td>0.738674</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.461078</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.793067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.653700</td>\n",
       "      <td>0.625921</td>\n",
       "      <td>0.415205</td>\n",
       "      <td>0.427711</td>\n",
       "      <td>0.403409</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.506772</td>\n",
       "      <td>0.411111</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.807773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.437322</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.516304</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.432700</td>\n",
       "      <td>0.393740</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.581522</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.867647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.365100</td>\n",
       "      <td>0.373573</td>\n",
       "      <td>0.606232</td>\n",
       "      <td>0.604520</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.883403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.335183</td>\n",
       "      <td>0.653521</td>\n",
       "      <td>0.648045</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.887605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.286025</td>\n",
       "      <td>0.771014</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.919118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.232100</td>\n",
       "      <td>0.275424</td>\n",
       "      <td>0.793003</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.925420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.250974</td>\n",
       "      <td>0.817391</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.930672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.203500</td>\n",
       "      <td>0.232603</td>\n",
       "      <td>0.827195</td>\n",
       "      <td>0.824859</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.934874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.266538</td>\n",
       "      <td>0.786982</td>\n",
       "      <td>0.820988</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.918067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.140800</td>\n",
       "      <td>0.304477</td>\n",
       "      <td>0.783383</td>\n",
       "      <td>0.819876</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.904412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.139300</td>\n",
       "      <td>0.289451</td>\n",
       "      <td>0.791789</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.909664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.233070</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.933824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.216231</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.943277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.396687</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.903361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.200530</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.944328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.294084</td>\n",
       "      <td>0.793003</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.911765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.085100</td>\n",
       "      <td>0.183269</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.870056</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.957983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.311845</td>\n",
       "      <td>0.818991</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.912815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.226164</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.812155</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.940126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.342937</td>\n",
       "      <td>0.828402</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.919118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.343193</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.915966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.050200</td>\n",
       "      <td>0.241639</td>\n",
       "      <td>0.835735</td>\n",
       "      <td>0.847953</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.935924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.241219</td>\n",
       "      <td>0.829971</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.936975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.263512</td>\n",
       "      <td>0.851312</td>\n",
       "      <td>0.874251</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.933824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.330464</td>\n",
       "      <td>0.839650</td>\n",
       "      <td>0.862275</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.902311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.258483</td>\n",
       "      <td>0.834783</td>\n",
       "      <td>0.852071</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.936975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.305845</td>\n",
       "      <td>0.844575</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.924370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.381393</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.898109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.346093</td>\n",
       "      <td>0.825959</td>\n",
       "      <td>0.858896</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.901261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.348949</td>\n",
       "      <td>0.824926</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.894958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.341784</td>\n",
       "      <td>0.831858</td>\n",
       "      <td>0.865031</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.900210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.903361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.343707</td>\n",
       "      <td>0.822157</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.902311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aweso\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=440, training_loss=0.2875511886043982, metrics={'train_runtime': 459.7339, 'train_samples_per_second': 7.222, 'train_steps_per_second': 0.957, 'total_flos': 550516933306758.0, 'train_loss': 0.2875511886043982, 'epoch': 40.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"ner-finetuned-amharic\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0182baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to 'ner-finetuned-amharic-final'\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"ner-finetuned-amharic-final\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"ner-finetuned-amharic-final\")\n",
    "\n",
    "print(\"Model and tokenizer saved to 'ner-finetuned-amharic-final'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b1aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(text, model, tokenizer, id2label):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", is_split_into_words=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    word_ids = inputs[\"input_ids\"].new_zeros(inputs[\"input_ids\"].shape[1]).tolist()\n",
    "    if hasattr(tokenizer, \"word_ids\"):\n",
    "        word_ids = tokenizer(text, return_tensors=\"pt\").word_ids(batch_index=0)\n",
    "    else:\n",
    "        # fallback: treat each token as a word\n",
    "        word_ids = list(range(len(tokens)))\n",
    "    results = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is None or word_id == previous_word_idx:\n",
    "            continue\n",
    "        token = tokens[idx]\n",
    "        label = id2label[predictions[idx]]\n",
    "        results.append((token, label))\n",
    "        previous_word_idx = word_id\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53becad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long: B-Product\n",
      "-: I-Product\n",
      "lasting: I-Product\n",
      "battery: I-Product\n",
      "5000: B-PRICE\n",
      "ብር: I-PRICE\n",
      "መ: B-LOC\n",
      "ገና: B-LOC\n",
      "ኛ: B-LOC\n",
      "መሰ: I-LOC\n",
      "ረት: I-LOC\n",
      "_: I-LOC\n",
      "ደ: I-LOC\n",
      "ፋር: I-LOC\n",
      "_: I-LOC\n",
      "ሞ: I-LOC\n",
      "ል: I-LOC\n",
      "_: I-LOC\n",
      "ሁለተኛ: I-LOC\n",
      "_: I-LOC\n",
      "ፎ: I-LOC\n",
      "ቅ: I-LOC\n",
      "ላይ: I-LOC\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def pretty_print_ner(text, model, tokenizer, id2label):\n",
    "    results = predict_ner(text, model, tokenizer, id2label)\n",
    "    for token, label in results:\n",
    "        # Skip special tokens and empty tokens\n",
    "        if token in [\"<s>\", \"</s>\", \"▁\"] or label == \"O\":\n",
    "            continue\n",
    "        # Remove the leading '▁' for readability\n",
    "        print(f\"{token.replace('▁', '')}: {label}\")\n",
    "\n",
    "sample_text = \"ይህ አዲስ Long-lasting battery በ 5000 ብር ይሸጣል መገናኛ_መሰረት_ደፋር_ሞል_ሁለተኛ_ፎቅ ላይ ያገኙት።\"\n",
    "pretty_print_ner(sample_text, model, tokenizer, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90eac652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.343707412481308, 'eval_f1': 0.8221574344023324, 'eval_precision': 0.844311377245509, 'eval_recall': 0.8011363636363636, 'eval_accuracy': 0.9023109243697479, 'eval_runtime': 0.4651, 'eval_samples_per_second': 45.152, 'eval_steps_per_second': 6.45, 'epoch': 40.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efcea7",
   "metadata": {},
   "source": [
    "new model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10fa9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec63c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd63a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd869265a13b46f59c496b45859cb345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29760dc07abe4f14b88c039a4f82deb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6377e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-finetuned-bert-multilingual\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=40,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3611ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.argmax(torch.tensor(predictions), axis=2).tolist()\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dec10f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aweso\\AppData\\Local\\Temp\\ipykernel_44800\\3378667220.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [440/440 05:00, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.688100</td>\n",
       "      <td>1.359956</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.056818</td>\n",
       "      <td>0.619748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>1.011481</td>\n",
       "      <td>0.299595</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.210227</td>\n",
       "      <td>0.675420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.773139</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.746849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.622548</td>\n",
       "      <td>0.575163</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.805672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.477925</td>\n",
       "      <td>0.604230</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.837185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.430517</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.856092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.385667</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>0.769784</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.319767</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.678161</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.894958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.315125</td>\n",
       "      <td>0.687861</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.893908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.260179</td>\n",
       "      <td>0.752239</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.253000</td>\n",
       "      <td>0.308589</td>\n",
       "      <td>0.731988</td>\n",
       "      <td>0.742690</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.897059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.263580</td>\n",
       "      <td>0.741840</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>0.914916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.246581</td>\n",
       "      <td>0.740525</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.721591</td>\n",
       "      <td>0.920168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.228418</td>\n",
       "      <td>0.774929</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.932773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.185800</td>\n",
       "      <td>0.228716</td>\n",
       "      <td>0.755043</td>\n",
       "      <td>0.766082</td>\n",
       "      <td>0.744318</td>\n",
       "      <td>0.920168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.221159</td>\n",
       "      <td>0.790831</td>\n",
       "      <td>0.797688</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.165500</td>\n",
       "      <td>0.219825</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.788571</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.929622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.217346</td>\n",
       "      <td>0.787356</td>\n",
       "      <td>0.796512</td>\n",
       "      <td>0.778409</td>\n",
       "      <td>0.929622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.238584</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.920168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.263625</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.920168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.267586</td>\n",
       "      <td>0.764368</td>\n",
       "      <td>0.773256</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0.912815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.269186</td>\n",
       "      <td>0.743662</td>\n",
       "      <td>0.737430</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.907563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.204408</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.938025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.250562</td>\n",
       "      <td>0.795518</td>\n",
       "      <td>0.784530</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.925420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.215614</td>\n",
       "      <td>0.804533</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.231338</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.219020</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.932773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.224631</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.234135</td>\n",
       "      <td>0.817927</td>\n",
       "      <td>0.806630</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.217923</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.934874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.243573</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.260252</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.795580</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.925420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.066800</td>\n",
       "      <td>0.252978</td>\n",
       "      <td>0.815864</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.930672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.233980</td>\n",
       "      <td>0.811268</td>\n",
       "      <td>0.804469</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.929622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.225731</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.810056</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.932773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.238393</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.930672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.071200</td>\n",
       "      <td>0.235464</td>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.246307</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.811111</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.253180</td>\n",
       "      <td>0.819209</td>\n",
       "      <td>0.814607</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.929622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.253337</td>\n",
       "      <td>0.807910</td>\n",
       "      <td>0.803371</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.927521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=440, training_loss=0.2650248642672192, metrics={'train_runtime': 301.0764, 'train_samples_per_second': 11.027, 'train_steps_per_second': 1.461, 'total_flos': 538477930553406.0, 'train_loss': 0.2650248642672192, 'epoch': 40.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb19dc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ner-finetuned-bert-multilingual\\\\tokenizer_config.json',\n",
       " './ner-finetuned-bert-multilingual\\\\special_tokens_map.json',\n",
       " './ner-finetuned-bert-multilingual\\\\vocab.txt',\n",
       " './ner-finetuned-bert-multilingual\\\\added_tokens.json',\n",
       " './ner-finetuned-bert-multilingual\\\\tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./ner-finetuned-bert-multilingual\")\n",
    "tokenizer.save_pretrained(\"./ner-finetuned-bert-multilingual\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
