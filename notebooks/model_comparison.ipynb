{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c4060f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "489681e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded XLM-RoBERTa\n",
      " Loaded BERT-Multilingual\n"
     ]
    }
   ],
   "source": [
    "model_paths = {\n",
    "    \"XLM-RoBERTa\": \"./ner-finetuned-amharic-final\",\n",
    "    \"BERT-Multilingual\": \"./ner-finetuned-bert-multilingual\"\n",
    "}\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(path)\n",
    "        models[name] = model\n",
    "        tokenizers[name] = tokenizer\n",
    "        print(f\" Loaded {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d15f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        tokens = example[\"tokens\"]\n",
    "        word_labels = example[\"ner_tags\"]\n",
    "\n",
    "        encoding = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "        word_ids = encoding.word_ids(0)\n",
    "        inputs = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "\n",
    "        predicted_ids = predictions[0].cpu().tolist()\n",
    "\n",
    "        true_label_seq = []\n",
    "        pred_label_seq = []\n",
    "\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                continue\n",
    "            if word_idx >= len(word_labels):\n",
    "                continue\n",
    "\n",
    "            #  FIX: check label type before indexing\n",
    "            true_label = word_labels[word_idx] if isinstance(word_labels[word_idx], str) else id2label[word_labels[word_idx]]\n",
    "            pred_label = id2label[predicted_ids[i]]\n",
    "\n",
    "            true_label_seq.append(true_label)\n",
    "            pred_label_seq.append(pred_label)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        true_labels.append(true_label_seq)\n",
    "        pred_labels.append(pred_label_seq)\n",
    "\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "        \"precision\": precision_score(true_labels, pred_labels),\n",
    "        \"recall\": recall_score(true_labels, pred_labels),\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44b6110f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating XLM-RoBERTa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 42.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XLM-RoBERTa Scores:\n",
      "{'f1': np.float64(0.8221574344023324), 'precision': np.float64(0.844311377245509), 'recall': np.float64(0.8011363636363636), 'accuracy': 0.9023109243697479}\n",
      "\n",
      " Evaluating BERT-Multilingual...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 81.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BERT-Multilingual Scores:\n",
      "{'f1': np.float64(0.8079096045197739), 'precision': np.float64(0.8033707865168539), 'recall': np.float64(0.8125), 'accuracy': 0.9275210084033614}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name in models:\n",
    "    print(f\"\\n Evaluating {name}...\")\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    try:\n",
    "        scores = evaluate_model(models[name], tokenizers[name], dataset[\"test\"])\n",
    "        results[name] = scores\n",
    "        print(f\" {name} Scores:\\n{scores}\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\" KeyError while evaluating {name}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f5cc9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Measuring inference time and size for XLM-RoBERTa...\n",
      " XLM-RoBERTa - Avg Inference Time: 0.0115s, Model Size: 1058.43 MB\n",
      "\n",
      " Measuring inference time and size for BERT-Multilingual...\n",
      " BERT-Multilingual - Avg Inference Time: 0.0125s, Model Size: 676.24 MB\n",
      "\n",
      "=== Combined Model Comparison ===\n",
      "\n",
      "Model: XLM-RoBERTa\n",
      "F1 Score: 0.8222\n",
      "Precision: 0.8443\n",
      "Recall: 0.8011\n",
      "Accuracy: 0.9023\n",
      "Avg Inference Time (s): 0.0115\n",
      "Model Size (MB): 1058.43\n",
      "\n",
      "Model: BERT-Multilingual\n",
      "F1 Score: 0.8079\n",
      "Precision: 0.8034\n",
      "Recall: 0.8125\n",
      "Accuracy: 0.9275\n",
      "Avg Inference Time (s): 0.0125\n",
      "Model Size (MB): 676.24\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_inference_time(model, tokenizer, dataset, max_samples=50):\n",
    "    \"\"\"\n",
    "    Measure average inference time per sample on the given dataset (limited by max_samples).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    total_time = 0\n",
    "    count = 0\n",
    "\n",
    "    for example in dataset.select(range(min(len(dataset), max_samples))):\n",
    "        tokens = example[\"tokens\"]\n",
    "        encoding = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "        count += 1\n",
    "\n",
    "    avg_time = total_time / count if count > 0 else 0\n",
    "    return avg_time\n",
    "\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"\n",
    "    Returns model size in megabytes (approximate, parameters only).\n",
    "    \"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    # Assuming 4 bytes per parameter (float32)\n",
    "    size_mb = param_count * 4 / (1024 ** 2)\n",
    "    return size_mb\n",
    "\n",
    "# Run measurement for all models\n",
    "speed_and_size_results = {}\n",
    "\n",
    "for name in models:\n",
    "    print(f\"\\n Measuring inference time and size for {name}...\")\n",
    "    avg_inference_time = measure_inference_time(models[name], tokenizers[name], dataset[\"test\"])\n",
    "    model_size_mb = get_model_size_mb(models[name])\n",
    "    \n",
    "    speed_and_size_results[name] = {\n",
    "        \"avg_inference_time_sec\": avg_inference_time,\n",
    "        \"model_size_mb\": model_size_mb\n",
    "    }\n",
    "    print(f\" {name} - Avg Inference Time: {avg_inference_time:.4f}s, Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Display combined results: metrics + speed + size\n",
    "print(\"\\n=== Combined Model Comparison ===\")\n",
    "for name in results:\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"F1 Score: {results[name]['f1']:.4f}\")\n",
    "    print(f\"Precision: {results[name]['precision']:.4f}\")\n",
    "    print(f\"Recall: {results[name]['recall']:.4f}\")\n",
    "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"Avg Inference Time (s): {speed_and_size_results[name]['avg_inference_time_sec']:.4f}\")\n",
    "    print(f\"Model Size (MB): {speed_and_size_results[name]['model_size_mb']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "178d64b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Comparison & Selection Summary \n",
      "\n",
      "Model: XLM-RoBERTa\n",
      "  - F1 Score: 0.8222\n",
      "  - Precision: 0.8443\n",
      "  - Recall: 0.8011\n",
      "  - Accuracy: 0.9023\n",
      "  - Average Inference Time (sec): 0.0115\n",
      "  - Model Size (MB): 1058.43\n",
      "\n",
      "Model: BERT-Multilingual\n",
      "  - F1 Score: 0.8079\n",
      "  - Precision: 0.8034\n",
      "  - Recall: 0.8125\n",
      "  - Accuracy: 0.9275\n",
      "  - Average Inference Time (sec): 0.0125\n",
      "  - Model Size (MB): 676.24\n",
      "\n",
      "Recommendation:\n",
      "Based on the evaluation metrics and resource considerations, the XLM-RoBERTa model offers slightly better F1 score and precision, but it is significantly larger in size (~1GB). The BERT-Multilingual model provides a more compact solution with comparable performance and better recall and accuracy. For deployment scenarios where resources are limited or faster loading times are critical, BERT-Multilingual is recommended. However, if the priority is maximizing precision and F1 score without strict size constraints, XLM-RoBERTa is preferred.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Model Comparison & Selection Summary \\n\")\n",
    "\n",
    "for name in results:\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"  - F1 Score: {results[name]['f1']:.4f}\")\n",
    "    print(f\"  - Precision: {results[name]['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {results[name]['recall']:.4f}\")\n",
    "    print(f\"  - Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  - Average Inference Time (sec): {speed_and_size_results[name]['avg_inference_time_sec']:.4f}\")\n",
    "    print(f\"  - Model Size (MB): {speed_and_size_results[name]['model_size_mb']:.2f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Recommendation:\")\n",
    "print(\n",
    "    \"Based on the evaluation metrics and resource considerations, \"\n",
    "    \"the XLM-RoBERTa model offers slightly better F1 score and precision, \"\n",
    "    \"but it is significantly larger in size (~1GB). \"\n",
    "    \"The BERT-Multilingual model provides a more compact solution with comparable performance and better recall and accuracy. \"\n",
    "    \"For deployment scenarios where resources are limited or faster loading times are critical, BERT-Multilingual is recommended. \"\n",
    "    \"However, if the priority is maximizing precision and F1 score without strict size constraints, XLM-RoBERTa is preferred.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
